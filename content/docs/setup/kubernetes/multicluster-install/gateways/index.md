---
title: Gateway connectivity
description: Install an Istio mesh across multiple Kubernetes clusters using Istio Gateway to reach remote pods.
weight: 2
keywords: [kubernetes,multicluster,federation,gateway]
---

Instructions for installing an Istio mesh across multiple clusters when pods
in each cluster can only connect to remote gateway IPs.

Instead of using a central Istio control plane to manage the mesh,
in this configuration each cluster has an **identical** Istio control plane
installation, each managing its own endpoints.
All of the clusters are under a shared administrative control for the purposes of
policy enforcement and security.

A single Istio service mesh across the clusters is achieved by replicating
shared services and namespaces and using a common root CA in all of the clusters.
Cross-cluster communication occurs over Istio Gateways of the respective clusters.

{{< image width="80%" ratio="36.01%"
    link="./multicluster-with-gateways.svg"
    caption="Istio mesh spanning multiple Kubernetes clusters using Istio Gateway to reach remote pods"
    >}}

## Prerequisites

* Two or more Kubernetes clusters with **1.10 or newer**.

* Authority to deploy the [Istio control plane using Helm](/docs/setup/kubernetes/helm-install/)
on **each** Kubernetes cluster.

* The IP address of the `istio-ingressgateway` service in each cluster must
  be accessible from every other cluster.

* A **Root CA**. Cross cluster communication requires mTLS connection
  between services. To enable mTLS communication across clusters, each
  cluster's Citadel will be configured with intermediate CA credentials
  generated by a shared root CA. For illustration purposes, we use a
  sample root CA certificate available as part of Istio install
  under the `samples/certs` directory.

## Deploy Istio control plane in each cluster

1. Generate intermediate CA certs for each cluster's Citadel from your
organization's root CA. The shared root CA enables mTLS communication
across different clusters. For illustration purposes, we use
the sample root certificates as the intermediate certificate.

1. In every cluster, create a Kubernetes secret for your generated CA certs
   using a command similar to the following:

    {{< text bash >}}
    $ kubectl create namespace istio-system
    $ kubectl create secret generic cacerts -n istio-system \
        --from-file=samples/certs/ca-cert.pem \
        --from-file=samples/certs/ca-key.pem \
        --from-file=samples/certs/root-cert.pem \
        --from-file=samples/certs/cert-chain.pem
    {{< /text >}}

1. Install the Istio control plane in every cluster using the following commands:

    {{< text bash >}}
    $ helm template install/kubernetes/helm/istio --name istio --namespace istio-system \
        -f install/kubernetes/helm/istio/values-istio-multicluster-gateways.yaml > $HOME/istio.yaml
    $ kubectl create namespace istio-system
    $ kubectl apply -f $HOME/istio.yaml
    {{< /text >}}

For further details and customization options, refer to the [Installation
with Helm](/docs/setup/kubernetes/helm-install/) instructions.

## Configure DNS

Providing a DNS resolution for services in remote clusters will allow
existing applications to function unmodified, as applications typically
expect to resolve services by their DNS names and access the resulting
IP. Istio itself does not use the DNS for routing requests between
services. Services local to a cluster share a common DNS suffix
(e.g., `svc.cluster.local`). Kubernetes DNS provides DNS resolution for these
services.

To provide a similar setup for services from remote clusters, we name
services from remote clusters in the format
`<name>.<namespace>.global`. Istio also ships with a CoreDNS server that
will provide DNS resolution for these services. In order to utilize this
DNS, Kubernetes' DNS needs to be configured to point to CoreDNS as the DNS
server for the `.global` DNS domain. Create the following ConfigMap (or
update an existing one):

{{< text bash >}}
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  stubDomains: |
    {"global": ["$(kubectl get svc -n istio-system istiocoredns -o jsonpath={.spec.clusterIP})"]}
EOF
{{< /text >}}

## Setup Steps

1. Deploy `httpbin` service in cluster1.
{{< text bash >}}

$ kubectl create namespace bar
$ kubectl apply -f <(bin/istioctl kube-inject -f samples/httpbin/httpbin.yaml) -n bar
$ export GATEWAY_IP_CLUSTER1=$(kubectl get svc --selector=app=istio-ingressgateway \
    -n istio-system -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}")
{{< /text >}}

1. Deploy `sleep` service in cluster2.
{{< text bash >}}

$ kubectl create namespace foo
$ kubectl apply -f <(bin/istioctl kube-inject -f samples/sleep/sleep.yaml) -n foo
{{< /text >}}

1. Create `ServiceEntry` for httpbin service in cluster2.

    To allow `sleep` in cluster2 access `httpbin` in cluster1, we need to create
    `ServiceEntry` for that. Host name of the `ServiceEntry` should be of the form
    `<name>.<namespace>.global` where name and namespace correspond to the
    remote service's name and namespace respectively.

    For DNS resolution for services under the
    `*.global` domain, you need to assign these services an IP address. We
    suggest assigning an IP address from the 127.255.0.0/16 subnet. These IPs
    are non-routable outside of a pod. Application traffic for these IPs will
    be captured by the sidecar and routed to the appropriate remote service.

    > Each service (in the .global DNS domain) must have a unique IP within the
    cluster.

    {{< text bash >}}

    $ kubectl -n foo apply -f - <<EOF
    apiVersion: networking.istio.io/v1alpha3
    kind: ServiceEntry
    metadata:
      name: httpbin-bar
    spec:
      hosts:
      # must be of form name.namespace.global
      - httpbin.bar.global
      # Treat remote cluster services as part of the service mesh
      # as all clusters in the service mesh share the same root of trust.
      location: MESH_INTERNAL
      ports:
      - name: http1
        number: 8000
        protocol: http
      resolution: DNS
      addresses:
      # the IP address to which httpbin.bar.global will resolve to
      # must be unique for each remote service, within a given cluster.
      # This address need not be routable. Traffic for this IP will be captured
      # by the sidecar and routed appropriately.
      - 127.255.0.2
      endpoints:
      # This is the routable address of the ingress gateway in cluster2 that
      # sits in front of sleep.bar service. Traffic from the sidecar will be
      # routed to this address.
      - address: ${GATEWAY_IP_CLUSTER1}
        ports:
          http1: 15443 # Do not change this port value
    EOF
    {{< /text >}}

    The configurations above will result in all traffic in `cluster1` for
    `httpbin.bar.global` on *any port* to be routed to the endpoint
    `<IPofCluster2IngressGateway>:15443` over an mTLS connection.

    > Do not create a Gateway configuration for port 15443.

    The gateway port 15443 is a special SNI-aware Envoy
    preconfigured and installed as part of the Istio installation step
    described in the prerequisite section.  Traffic entering port 15443 will be
    load balanced among pods of the appropriate internal service of the target
    cluster (in this case, `httpbin.bar` in cluster1).

1. Very Access `httpbin` is accessible from `sleep`.
{{< text bash >}}
$ kubectl exec $(kubectl -n foo get pod -l app=sleep -o jsonpath={.items..metadata.name}) \
   -n foo -c sleep -- curl httpbin.bar.global:8000/ip
{{< /text >}}

## Force remote cluster traffic through Egress Gateway

If you wish to route all egress traffic from `cluster2` via a dedicated
egress gateway, use the following service entry for `httpbin.bar`.

{{< text bash >}}
$ kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: httpbin-bar
spec:
  hosts:
  # must be of form name.namespace.global
  - httpbin.bar.global
  location: MESH_INTERNAL
  ports:
  - name: http1
    number: 8000
    protocol: http
  resolution: DNS
  addresses:
  - 127.255.0.2
  endpoints:
  - address: ${GATEWAY_IP_CLUSTER1}
    network: external
    ports:
      http1: 15443 # Do not change this port value
  - address: istio-egressgateway.istio-system.svc.cluster.local
    ports:
      http1: 15443
EOF
{{< /text >}}

## Version-aware routing to remote services

If the remote service being added has multiple versions, add one or more
labels to the service entry endpoint, and follow the steps outlined in the
[request routing](/docs/tasks/traffic-management/request-routing/) section
to create appropriate virtual services and destination rules. For example,

{{< text bash >}}
$ kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: httpbin-bar
spec:
  hosts:
  # must be of form name.namespace.global
  - httpbin.bar.global
  location: MESH_INTERNAL
  ports:
  - name: http1
    number: 8000
    protocol: http
  resolution: DNS
  addresses:
  # the IP address to which httpbin.bar.global will resolve to
  # must be unique for each service.
  - 127.255.0.2
  endpoints:
  - address: ${GATEWAY_IP_CLUSTER1}
    labels:
      version: beta
      some: thing
      foo: bar
    ports:
      http1: 15443 # Do not change this port value
EOF
{{< /text >}}

Use destination rules to create subsets for `httpbin.bar` service with
appropriate label selectors. The set of steps to follow are identical to
those used for a local service.

## Summary

Using Istio gateways, a common root CA, and `ServiceEntry`, you configured
a single Istio service mesh across multiple Kubernetes clusters.  Although
the above procedure involved a certain amount of manual work, the entire
process could be automated by creating service entries for each service in
the system, with a unique IP allocated from the `127.255.0.0/16` subnet. Once
configured this way, traffic can be transparently routed to remote clusters
without any application involvement.
